{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbCjylAIegNdQ/tcTBTljr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaharika/DSA0311--Natural-Language-Processing/blob/main/NLP_Programs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write program demonstrates how to use regular expressions in Python to match and search for patterns in text."
      ],
      "metadata": {
        "id": "nvrdrafQH92b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def find_emails(text):\n",
        "    # Define a simple regular expression for matching email addresses\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "\n",
        "    # Use re.findall to find all matches in the text\n",
        "    matches = re.findall(email_pattern, text)\n",
        "\n",
        "    return matches\n",
        "\n",
        "# Example text containing email addresses\n",
        "sample_text = \"Contact us at info@example.com or support@company.com for assistance.\"\n",
        "\n",
        "# Find and print all email addresses in the text\n",
        "email_addresses = find_emails(sample_text)\n",
        "print(\"Email Addresses Found:\")\n",
        "print(email_addresses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CevmaxmdIUpD",
        "outputId": "a847a2ea-9abe-4777-e7fa-3846e515fe9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email Addresses Found:\n",
            "['info@example.com', 'support@company.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement a basic finite state automaton that recognizes a specific language or pattern. In this example, we'll create a simple automaton to match strings ending with 'ab' using python."
      ],
      "metadata": {
        "id": "-fonIsYxIe06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_match(input_string):\n",
        "    # Define the finite state automaton transitions\n",
        "    transitions = {\n",
        "        0: {'a': 1, 'b': 0},\n",
        "        1: {'a': 1, 'b': 2},\n",
        "        2: {'a': 1, 'b': 0}\n",
        "    }\n",
        "    current_state = 0\n",
        "    # Process each character in the input string\n",
        "    for char in input_string:\n",
        "        if char in transitions[current_state]:\n",
        "            current_state = transitions[current_state][char]\n",
        "        else:\n",
        "            # If there is no transition for the current character, reset to the initial state\n",
        "            current_state = 0\n",
        "    # Check if the final state is reached\n",
        "    return current_state == 2\n",
        "# Test the automaton with various strings\n",
        "test_strings = [\"ab\", \"aab\", \"aaaab\", \"abc\", \"xyzab\", \"abab\", \"ba\"]\n",
        "for test_string in test_strings:\n",
        "    if is_match(test_string):\n",
        "        print(f\"'{test_string}' matches the pattern.\")\n",
        "    else:\n",
        "        print(f\"'{test_string}' does not match the pattern.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_rLMiJfImUM",
        "outputId": "b62f210b-3f42-4950-a9d9-cc2673462d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'ab' matches the pattern.\n",
            "'aab' matches the pattern.\n",
            "'aaaab' matches the pattern.\n",
            "'abc' does not match the pattern.\n",
            "'xyzab' matches the pattern.\n",
            "'abab' matches the pattern.\n",
            "'ba' does not match the pattern.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write program demonstrates how to perform morphological analysis using the NLTK library in Python."
      ],
      "metadata": {
        "id": "ZAbuNZkpIpiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')  # Download the punkt tokenizer if not already downloaded\n",
        "\n",
        "def perform_morphological_analysis(text):\n",
        "    # Tokenize the input text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Create a Porter stemmer object\n",
        "    porter_stemmer = PorterStemmer()\n",
        "\n",
        "    # Perform stemming on each word\n",
        "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "    return stemmed_words\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example text for morphological analysis\n",
        "    input_text = \"The quick brown foxes are jumping over the lazy dogs\"\n",
        "\n",
        "    # Perform morphological analysis (stemming)\n",
        "    result = perform_morphological_analysis(input_text)\n",
        "\n",
        "    # Display the original and stemmed words\n",
        "    print(\"Original words:\", nltk.word_tokenize(input_text))\n",
        "    print(\"Stemmed words:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x27Kp3MIvGL",
        "outputId": "179d6a34-0056-4db0-aba4-ec1c81b8bc86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs']\n",
            "Stemmed words: ['the', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazi', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Implement a finite-state machine for morphological parsing. In this example, we'll create a simple machine to generate plural forms of English nouns using python."
      ],
      "metadata": {
        "id": "cC6IOzF_I4Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag, word_tokenize\n",
        "def identify_nouns(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    tagged_words = pos_tag(words)\n",
        "    print(tagged_words)\n",
        "\n",
        "    nouns = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
        "\n",
        "    return nouns\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "nouns = identify_nouns(sentence)\n",
        "\n",
        "if nouns:\n",
        "    print(\"Nouns identified in the sentence:\")\n",
        "    for noun in nouns:\n",
        "        if noun[-1].lower() in {'s', 'x', 'z'} or noun[-2:].lower() in {'ch', 'sh'}:\n",
        "            print(noun+\"es\")\n",
        "        else:\n",
        "            print(noun+\"s\")\n",
        "else:\n",
        "    print(\"No nouns found in the sentence.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZOA_DyVI86O",
        "outputId": "7491e132-09ec-4d06-ec31-7787b3d15b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
            "Nouns identified in the sentence:\n",
            "browns\n",
            "foxes\n",
            "dogs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Use the Porter Stemmer algorithm to perform word stemming on a list of words using python libraries."
      ],
      "metadata": {
        "id": "qABzsqExJBjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "def perform_stemming(words):\n",
        "    # Initialize the Porter Stemmer\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    # Perform stemming for each word\n",
        "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "words_to_stem = [\"running\", \"jumps\", \"happily\", \"dogs\", \"cats\", \"better\"]\n",
        "\n",
        "stemmed_words = perform_stemming(words_to_stem)\n",
        "\n",
        "print(\"Original Words:\", words_to_stem)\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zslL-b1FJGUC",
        "outputId": "07865f23-5879-486d-ee7f-c8f34089295e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['running', 'jumps', 'happily', 'dogs', 'cats', 'better']\n",
            "Stemmed Words: ['run', 'jump', 'happili', 'dog', 'cat', 'better']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Implement a basic N-gram model for text generation. For example, generate text using a bigram model using python."
      ],
      "metadata": {
        "id": "ukedbR0qJg3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def build_bigram_model(sentences):\n",
        "    bigram_model = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = sentence.split()\n",
        "        for i in range(len(tokens) - 1):\n",
        "            current_word = tokens[i]\n",
        "            next_word = tokens[i + 1]\n",
        "\n",
        "            if current_word in bigram_model:\n",
        "\n",
        "                bigram_model[current_word].append(next_word)\n",
        "            else:\n",
        "                bigram_model[current_word] = [next_word]\n",
        "\n",
        "    return bigram_model\n",
        "\n",
        "def generate_text(bigram_model, start_word, length=10):\n",
        "    generated_text = [start_word]\n",
        "\n",
        "    for _ in range(length - 1):\n",
        "        if start_word in bigram_model:\n",
        "            next_word = random.choice(bigram_model[start_word])\n",
        "            generated_text.append(next_word)\n",
        "            start_word = next_word\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "# Example list of sentences\n",
        "sentences = [\n",
        "    \"I love programming in Python.\",\n",
        "    \"Python is a versatile programming language.\",\n",
        "    \"Text generation using bigram models is interesting.\",\n",
        "    \"Natural Language Processing involves analyzing and generating text.\"\n",
        "]\n",
        "\n",
        "# Build bigram model\n",
        "bigram_model = build_bigram_model(sentences)\n",
        "print(bigram_model)\n",
        "\n",
        "# Generate text using bigram model\n",
        "generated_text = generate_text(bigram_model, start_word=\"I\", length=8)\n",
        "\n",
        "# Display the results\n",
        "print(\"Generated Text:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mio_T94JkUr",
        "outputId": "e4584dd9-1650-47ca-8700-c24d53bc64cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'I': ['love'], 'love': ['programming'], 'programming': ['in', 'language.'], 'in': ['Python.'], 'Python': ['is'], 'is': ['a', 'interesting.'], 'a': ['versatile'], 'versatile': ['programming'], 'Text': ['generation'], 'generation': ['using'], 'using': ['bigram'], 'bigram': ['models'], 'models': ['is'], 'Natural': ['Language'], 'Language': ['Processing'], 'Processing': ['involves'], 'involves': ['analyzing'], 'analyzing': ['and'], 'and': ['generating'], 'generating': ['text.']}\n",
            "Generated Text: I love programming language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write program using the NLTK library to perform part-of-speech tagging on a text."
      ],
      "metadata": {
        "id": "rB01XQ5EKHQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "def perform_pos_tagging(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "    # Perform part-of-speech tagging\n",
        "    tagged_words = pos_tag(words)\n",
        "    return tagged_words\n",
        "# Example text\n",
        "text = \"NLTK is a powerful library for natural language processing.\"\n",
        "# Perform part-of-speech tagging\n",
        "tagged_words = perform_pos_tagging(text)\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Part-of-Speech Tagging Result:\", tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGrzK-XUKLnb",
        "outputId": "59858c7c-c3fd-493c-9cfb-58b54b24b986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: NLTK is a powerful library for natural language processing.\n",
            "Part-of-Speech Tagging Result: [('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Implement a simple stochastic part-of-speech tagging algorithm using a basic probabilistic model to assign POS tags using python."
      ],
      "metadata": {
        "id": "eZJjpM9sqyEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def train_unigram_model(tagged_corpus):\n",
        "    unigram_model = {}\n",
        "\n",
        "    for sentence in tagged_corpus:\n",
        "        for word, pos_tag in sentence:\n",
        "            if word in unigram_model:\n",
        "                unigram_model[word].append(pos_tag)\n",
        "            else:\n",
        "                unigram_model[word] = [pos_tag]\n",
        "\n",
        "    return unigram_model\n",
        "\n",
        "def stochastic_pos_tagging(sentence, unigram_model):\n",
        "    tagged_sentence = []\n",
        "\n",
        "    for word in sentence:\n",
        "        if word in unigram_model:\n",
        "            pos_tag = random.choice(unigram_model[word])\n",
        "        else:\n",
        "            # If word not in model, assign a default POS tag (e.g., 'NOUN')\n",
        "            pos_tag = 'NOUN'\n",
        "\n",
        "        tagged_sentence.append((word, pos_tag))\n",
        "\n",
        "    return tagged_sentence\n",
        "\n",
        "# Example tagged corpus for training\n",
        "tagged_corpus = [\n",
        "    [('The', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN')],\n",
        "    [('Jumped', 'VERB'), ('over', 'PREP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n",
        "]\n",
        "\n",
        "# Train unigram model\n",
        "unigram_model = train_unigram_model(tagged_corpus)\n",
        "\n",
        "# Example sentence for stochastic POS tagging\n",
        "sentence_to_tag = ['The', 'lazy', 'fox', 'jumped']\n",
        "\n",
        "# Perform stochastic POS tagging\n",
        "tagged_sentence = stochastic_pos_tagging(sentence_to_tag, unigram_model)\n",
        "\n",
        "# Display the results\n",
        "print(\"Original Sentence:\", sentence_to_tag)\n",
        "print(\"Stochastic POS Tagging Result:\", tagged_sentence)"
      ],
      "metadata": {
        "id": "I3V0ZujRrFBM",
        "outputId": "019deefc-5350-4889-ed70-46dd421c1dc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: ['The', 'lazy', 'fox', 'jumped']\n",
            "Stochastic POS Tagging Result: [('The', 'DET'), ('lazy', 'ADJ'), ('fox', 'NOUN'), ('jumped', 'NOUN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Implement a rule-based part-of-speech tagging system using regular expressions using python."
      ],
      "metadata": {
        "id": "kc3OwijtrjFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def rule_based_pos_tagging(sentence):\n",
        "    tagged_sentence = []\n",
        "    for word in sentence:\n",
        "        if re.match(r'\\b(?:is|am|are|was|were)\\b', word, re.IGNORECASE):\n",
        "            pos_tag = 'VERB'\n",
        "        elif re.match(r'\\b(?:the|a|an)\\b', word, re.IGNORECASE):\n",
        "            pos_tag = 'DET'\n",
        "        elif re.match(r'\\b(?:quick|brown|lazy)\\b', word, re.IGNORECASE):\n",
        "            pos_tag = 'ADJ'\n",
        "        else:\n",
        "            pos_tag = 'NOUN'\n",
        "\n",
        "        tagged_sentence.append((word, pos_tag))\n",
        "    return tagged_sentence\n",
        "# Example sentence for rule-based POS tagging\n",
        "sentence_to_tag = ['The', 'quick', 'brown', 'fox', 'is', 'lazy']\n",
        "# Perform rule-based POS tagging\n",
        "tagged_sentence = rule_based_pos_tagging(sentence_to_tag)\n",
        "# Display the results\n",
        "print(\"Original Sentence:\", sentence_to_tag)\n",
        "print(\"Rule-based POS Tagging Result:\", tagged_sentence)"
      ],
      "metadata": {
        "id": "Vre4XjjUrouS",
        "outputId": "29d14ea1-b396-438b-ed8d-b6a80cdb805c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: ['The', 'quick', 'brown', 'fox', 'is', 'lazy']\n",
            "Rule-based POS Tagging Result: [('The', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('is', 'VERB'), ('lazy', 'ADJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Implement transformation-based tagging using a set of transformation rules, apply a simple rule to tag words using python."
      ],
      "metadata": {
        "id": "5sS-XauwruCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# Sample sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "words = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Define a set of custom rules for POS tagging\n",
        "custom_rules = [\n",
        "    (r'.*ing$', 'VBG'),      # Gerunds (e.g., running)\n",
        "    (r'.*ed$', 'VBD'),       # Past tense verbs (e.g., jumped)\n",
        "    (r'^[A-Z].*$', 'NNP'),   # Proper nouns (e.g., London)\n",
        "    (r'.*', 'NN')            # Default: Nouns for all other words\n",
        "]\n",
        "\n",
        "# Create a RegexpTagger with the custom rules\n",
        "regexp_tagger = nltk.RegexpTagger(custom_rules)\n",
        "\n",
        "# Tag the words using the RegexpTagger\n",
        "tags = regexp_tagger.tag(words)\n",
        "\n",
        "# Display the tagged words\n",
        "print(tags)"
      ],
      "metadata": {
        "id": "NoBgxt1bsBc5",
        "outputId": "60bbe149-beec-481d-9eea-fb2d5e264f1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'NNP'), ('quick', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NN'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Implement a simple top-down parser for context-free grammars using python.\n"
      ],
      "metadata": {
        "id": "bo1xbWF6J4rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleParser:\n",
        "    def __init__(self):\n",
        "        self.variables = {}\n",
        "\n",
        "    def parse(self, input_string):\n",
        "        self.input_string = input_string.split()\n",
        "        self.index = 0\n",
        "        self.current_token = None\n",
        "        self.next_token()\n",
        "        return self.statement()\n",
        "\n",
        "    def next_token(self):\n",
        "        if self.index < len(self.input_string):\n",
        "            self.current_token = self.input_string[self.index]\n",
        "            self.index += 1\n",
        "        else:\n",
        "            self.current_token = None\n",
        "\n",
        "    def match(self, token):\n",
        "        if self.current_token == token:\n",
        "            self.next_token()\n",
        "        else:\n",
        "            raise SyntaxError(\"Unexpected token: {}\".format(self.current_token))\n",
        "\n",
        "    def statement(self):\n",
        "        token = self.current_token\n",
        "        if token.isalpha():\n",
        "            var_name = token\n",
        "            self.next_token()\n",
        "            if self.current_token == '=':\n",
        "                self.next_token()\n",
        "                value = self.expr()\n",
        "                self.variables[var_name] = value\n",
        "                return value\n",
        "            else:\n",
        "                raise SyntaxError(\"Invalid statement: {}\".format(token))\n",
        "        else:\n",
        "            raise SyntaxError(\"Invalid statement: {}\".format(token))\n",
        "\n",
        "    def expr(self):\n",
        "        return self.term() + self.expr_tail()\n",
        "\n",
        "    def expr_tail(self):\n",
        "        if self.current_token in ('+', '-'):\n",
        "            op = self.current_token\n",
        "            self.next_token()\n",
        "            return op + self.term() + self.expr_tail()\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def term(self):\n",
        "        return self.factor() + self.term_tail()\n",
        "\n",
        "    def term_tail(self):\n",
        "        if self.current_token in ('*', '/'):\n",
        "            op = self.current_token\n",
        "            self.next_token()\n",
        "            return op + self.factor() + self.term_tail()\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def factor(self):\n",
        "        token = self.current_token\n",
        "        if token.isdigit():\n",
        "            self.next_token()\n",
        "            return token\n",
        "        elif token.isalpha():\n",
        "            if token in self.variables:\n",
        "                self.next_token()\n",
        "                return str(self.variables[token])\n",
        "            else:\n",
        "                raise SyntaxError(\"Undefined variable: {}\".format(token))\n",
        "        else:\n",
        "            raise SyntaxError(\"Invalid factor: {}\".format(token))\n",
        "\n",
        "\n",
        "# Example usage\n",
        "parser = SimpleParser()\n",
        "\n",
        "# Sample input\n",
        "input_program = \"\"\"\n",
        "x = 10\n",
        "y = 5\n",
        "result = x + y * 2\n",
        "\"\"\"\n",
        "\n",
        "# Parsing the input\n",
        "for line in input_program.split('\\n'):\n",
        "    if line.strip():\n",
        "        result = parser.parse(line.strip())\n",
        "        print(\"Parsed:\", line.strip(), \"->\", result)\n"
      ],
      "metadata": {
        "id": "Dh9SS_uoKRWQ",
        "outputId": "6620600d-5165-4825-dde5-27302048d71d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed: x = 10 -> 10\n",
            "Parsed: y = 5 -> 5\n",
            "Parsed: result = x + y * 2 -> 10+5*2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YlYDUs3uKQ9z"
      }
    }
  ]
}