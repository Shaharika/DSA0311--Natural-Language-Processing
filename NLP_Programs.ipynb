{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnsswGf/lx8OZwBDQMnNUs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaharika/DSA0311--Natural-Language-Processing/blob/main/NLP_Programs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write program demonstrates how to use regular expressions in Python to match and search for patterns in text."
      ],
      "metadata": {
        "id": "nvrdrafQH92b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def find_emails(text):\n",
        "    # Define a simple regular expression for matching email addresses\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "\n",
        "    # Use re.findall to find all matches in the text\n",
        "    matches = re.findall(email_pattern, text)\n",
        "\n",
        "    return matches\n",
        "\n",
        "# Example text containing email addresses\n",
        "sample_text = \"Contact us at info@example.com or support@company.com for assistance.\"\n",
        "\n",
        "# Find and print all email addresses in the text\n",
        "email_addresses = find_emails(sample_text)\n",
        "print(\"Email Addresses Found:\")\n",
        "print(email_addresses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CevmaxmdIUpD",
        "outputId": "a847a2ea-9abe-4777-e7fa-3846e515fe9a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email Addresses Found:\n",
            "['info@example.com', 'support@company.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement a basic finite state automaton that recognizes a specific language or pattern. In this example, we'll create a simple automaton to match strings ending with 'ab' using python."
      ],
      "metadata": {
        "id": "-fonIsYxIe06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_match(input_string):\n",
        "    # Define the finite state automaton transitions\n",
        "    transitions = {\n",
        "        0: {'a': 1, 'b': 0},\n",
        "        1: {'a': 1, 'b': 2},\n",
        "        2: {'a': 1, 'b': 0}\n",
        "    }\n",
        "    current_state = 0\n",
        "    # Process each character in the input string\n",
        "    for char in input_string:\n",
        "        if char in transitions[current_state]:\n",
        "            current_state = transitions[current_state][char]\n",
        "        else:\n",
        "            # If there is no transition for the current character, reset to the initial state\n",
        "            current_state = 0\n",
        "    # Check if the final state is reached\n",
        "    return current_state == 2\n",
        "# Test the automaton with various strings\n",
        "test_strings = [\"ab\", \"aab\", \"aaaab\", \"abc\", \"xyzab\", \"abab\", \"ba\"]\n",
        "for test_string in test_strings:\n",
        "    if is_match(test_string):\n",
        "        print(f\"'{test_string}' matches the pattern.\")\n",
        "    else:\n",
        "        print(f\"'{test_string}' does not match the pattern.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_rLMiJfImUM",
        "outputId": "b62f210b-3f42-4950-a9d9-cc2673462d5f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'ab' matches the pattern.\n",
            "'aab' matches the pattern.\n",
            "'aaaab' matches the pattern.\n",
            "'abc' does not match the pattern.\n",
            "'xyzab' matches the pattern.\n",
            "'abab' matches the pattern.\n",
            "'ba' does not match the pattern.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write program demonstrates how to perform morphological analysis using the NLTK library in Python."
      ],
      "metadata": {
        "id": "ZAbuNZkpIpiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')  # Download the punkt tokenizer if not already downloaded\n",
        "\n",
        "def perform_morphological_analysis(text):\n",
        "    # Tokenize the input text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Create a Porter stemmer object\n",
        "    porter_stemmer = PorterStemmer()\n",
        "\n",
        "    # Perform stemming on each word\n",
        "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "    return stemmed_words\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example text for morphological analysis\n",
        "    input_text = \"The quick brown foxes are jumping over the lazy dogs\"\n",
        "\n",
        "    # Perform morphological analysis (stemming)\n",
        "    result = perform_morphological_analysis(input_text)\n",
        "\n",
        "    # Display the original and stemmed words\n",
        "    print(\"Original words:\", nltk.word_tokenize(input_text))\n",
        "    print(\"Stemmed words:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x27Kp3MIvGL",
        "outputId": "179d6a34-0056-4db0-aba4-ec1c81b8bc86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs']\n",
            "Stemmed words: ['the', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazi', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Implement a finite-state machine for morphological parsing. In this example, we'll create a simple machine to generate plural forms of English nouns using python."
      ],
      "metadata": {
        "id": "cC6IOzF_I4Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag, word_tokenize\n",
        "def identify_nouns(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    tagged_words = pos_tag(words)\n",
        "    print(tagged_words)\n",
        "\n",
        "    nouns = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
        "\n",
        "    return nouns\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "nouns = identify_nouns(sentence)\n",
        "\n",
        "if nouns:\n",
        "    print(\"Nouns identified in the sentence:\")\n",
        "    for noun in nouns:\n",
        "        if noun[-1].lower() in {'s', 'x', 'z'} or noun[-2:].lower() in {'ch', 'sh'}:\n",
        "            print(noun+\"es\")\n",
        "        else:\n",
        "            print(noun+\"s\")\n",
        "else:\n",
        "    print(\"No nouns found in the sentence.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZOA_DyVI86O",
        "outputId": "7491e132-09ec-4d06-ec31-7787b3d15b77"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
            "Nouns identified in the sentence:\n",
            "browns\n",
            "foxes\n",
            "dogs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Use the Porter Stemmer algorithm to perform word stemming on a list of words using python libraries."
      ],
      "metadata": {
        "id": "qABzsqExJBjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "def perform_stemming(words):\n",
        "    # Initialize the Porter Stemmer\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    # Perform stemming for each word\n",
        "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "words_to_stem = [\"running\", \"jumps\", \"happily\", \"dogs\", \"cats\", \"better\"]\n",
        "\n",
        "stemmed_words = perform_stemming(words_to_stem)\n",
        "\n",
        "print(\"Original Words:\", words_to_stem)\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zslL-b1FJGUC",
        "outputId": "07865f23-5879-486d-ee7f-c8f34089295e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['running', 'jumps', 'happily', 'dogs', 'cats', 'better']\n",
            "Stemmed Words: ['run', 'jump', 'happili', 'dog', 'cat', 'better']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Implement a basic N-gram model for text generation. For example, generate text using a bigram model using python."
      ],
      "metadata": {
        "id": "ukedbR0qJg3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def build_bigram_model(sentences):\n",
        "    bigram_model = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = sentence.split()\n",
        "        for i in range(len(tokens) - 1):\n",
        "            current_word = tokens[i]\n",
        "            next_word = tokens[i + 1]\n",
        "\n",
        "            if current_word in bigram_model:\n",
        "\n",
        "                bigram_model[current_word].append(next_word)\n",
        "            else:\n",
        "                bigram_model[current_word] = [next_word]\n",
        "\n",
        "    return bigram_model\n",
        "\n",
        "def generate_text(bigram_model, start_word, length=10):\n",
        "    generated_text = [start_word]\n",
        "\n",
        "    for _ in range(length - 1):\n",
        "        if start_word in bigram_model:\n",
        "            next_word = random.choice(bigram_model[start_word])\n",
        "            generated_text.append(next_word)\n",
        "            start_word = next_word\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "# Example list of sentences\n",
        "sentences = [\n",
        "    \"I love programming in Python.\",\n",
        "    \"Python is a versatile programming language.\",\n",
        "    \"Text generation using bigram models is interesting.\",\n",
        "    \"Natural Language Processing involves analyzing and generating text.\"\n",
        "]\n",
        "\n",
        "# Build bigram model\n",
        "bigram_model = build_bigram_model(sentences)\n",
        "print(bigram_model)\n",
        "\n",
        "# Generate text using bigram model\n",
        "generated_text = generate_text(bigram_model, start_word=\"I\", length=8)\n",
        "\n",
        "# Display the results\n",
        "print(\"Generated Text:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mio_T94JkUr",
        "outputId": "e4584dd9-1650-47ca-8700-c24d53bc64cb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'I': ['love'], 'love': ['programming'], 'programming': ['in', 'language.'], 'in': ['Python.'], 'Python': ['is'], 'is': ['a', 'interesting.'], 'a': ['versatile'], 'versatile': ['programming'], 'Text': ['generation'], 'generation': ['using'], 'using': ['bigram'], 'bigram': ['models'], 'models': ['is'], 'Natural': ['Language'], 'Language': ['Processing'], 'Processing': ['involves'], 'involves': ['analyzing'], 'analyzing': ['and'], 'and': ['generating'], 'generating': ['text.']}\n",
            "Generated Text: I love programming language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write program using the NLTK library to perform part-of-speech tagging on a text."
      ],
      "metadata": {
        "id": "rB01XQ5EKHQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "def perform_pos_tagging(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "    # Perform part-of-speech tagging\n",
        "    tagged_words = pos_tag(words)\n",
        "    return tagged_words\n",
        "# Example text\n",
        "text = \"NLTK is a powerful library for natural language processing.\"\n",
        "# Perform part-of-speech tagging\n",
        "tagged_words = perform_pos_tagging(text)\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Part-of-Speech Tagging Result:\", tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGrzK-XUKLnb",
        "outputId": "59858c7c-c3fd-493c-9cfb-58b54b24b986"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: NLTK is a powerful library for natural language processing.\n",
            "Part-of-Speech Tagging Result: [('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YlYDUs3uKQ9z"
      }
    }
  ]
}